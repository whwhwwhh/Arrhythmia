{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde28e6f",
   "metadata": {},
   "source": [
    "### 1. Purpose\n",
    "The purpose of this document is to provide a set of problems to be solved by Vodafone Advanced Data Analytics\n",
    "candidates as a means of skill assessment.\n",
    "### 2. Instructions\n",
    "Solve the problems as directed below.\n",
    "\n",
    "#### 2.1. Code base\n",
    "- Python>=3.6\n",
    "- Include requirements.txt file in each repository\n",
    "\n",
    "### 2.2. Submission\n",
    "Each solution should be uploaded as a GitHub repository that will be deleted after\n",
    "assessment.\n",
    "\n",
    "Provide the repository links to repos in an email to mo.namazi@tpgtelecom.com.au and cc jiaxi.li@tpgtelecom.com.au / mahati.suvvari@tpgtelecom.com.au\n",
    "\n",
    "#### 2.3. Data\n",
    "The data for the problems can be found under the `data/` folder\n",
    "\n",
    "#### 2.4. Guidelines\n",
    "General solutions to these problems may be found available on the internet, feel free to\n",
    "leverage these, however keep in mind we are looking for out-of-the-box thinking as well as\n",
    "neat and scalable code.\n",
    "Focus on the areas you are skilled in.\n",
    "You will be asked to explain your code in full detail.\n",
    "This is an opportunity to show your skills, as much as pass a test, we do not have perfect\n",
    "solutions in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74720bcd",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "We provide an open source dataset for cardiac arrhythmia classification. The dataset contains 452 patients and 279 attributes\n",
    "\n",
    "The dataset is provided in 2 different files:\n",
    "- `arrhythmia.data`\n",
    "\n",
    "- `arrhythmia.names`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c88016",
   "metadata": {},
   "source": [
    "### Task 1 - Modelling\n",
    "\n",
    "Build a standalone python program (.py) that can be executed in command-line (such as Terminal, Powershell, etc.).\n",
    "\n",
    "`a.` Explore the dataset on the basic statistics, produce a classification model to predict the different classes of Arrhythmia, and present the findings as well as the performance of the model.\n",
    "\n",
    "---\n",
    "\n",
    "`Requirement.` **Using PySpark to build the application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3dc04b-d8fc-4d2a-b297-f04c4de6f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(path:str)-> None:\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c3308-e121-4054-b038-0589c2ddc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eee4e0-3dba-40f2-a8df-f260eeeffe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change threhold values - algorithm\n",
    "#reduce the amount of data for normal class.\n",
    "# low recall samples, keep not normal class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7a814-451c-4100-ae32-0e52561fafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model need to be flexible. training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08badff8-042e-4fc1-a2d2-01fa009bbd2f",
   "metadata": {},
   "source": [
    "1. compare model efficency binary classification model\n",
    "2. tune parameters. \n",
    "3. improve recall.\n",
    "4. data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7abbbf",
   "metadata": {},
   "source": [
    "### Task 2 - Deployment\n",
    "\n",
    "Build a controlled environment to package the above solution, so that this code and model could be easily executed in different platform and servers without manually resolving the dependencies/libraries.\n",
    "\n",
    "- Build a container environment/application that the training and inferencing workflow could be executed separated in CLI.\n",
    "- Make sure the application code is modular and easy to read by peers.\n",
    "\n",
    "Note that, it is important to keep in mind that, the same code need to be executed in our environemnt without changing much to the code submitted.\n",
    "\n",
    "---\n",
    "\n",
    "`Hint.` To complete this task, it is expected to include the detailed steps of executing the build of the container environment.\n",
    "\n",
    "`Requirement.` Using `spark-submit` to start/execute the scripts/application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b744455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#see app folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450d4bc",
   "metadata": {},
   "source": [
    "### Task 3 - Data Analytics\n",
    "\n",
    "Build interesting insights that you could find from the provided dataset and/or the predictive results.\n",
    "\n",
    "---\n",
    "\n",
    "`Hint.` Insights could be presented with or without the results from `Task 1`. Feel free to use any tools and libraries to visualise the insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37fbb1-c5e2-45d6-a137-6da2ea9d797e",
   "metadata": {},
   "source": [
    "#TODO\n",
    "1. age, heights, sex, weights, \n",
    "2. scalter, boxplot, obeserve data outliers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3645a52b-a017-4469-83c2-4ee3c1cb5622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/29 22:20:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/29 22:20:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/09/29 22:20:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "279\n",
       "1     0.542035\n",
       "10    0.110619\n",
       "2     0.097345\n",
       "6     0.055310\n",
       "16    0.048673\n",
       "3     0.033186\n",
       "4     0.033186\n",
       "5     0.028761\n",
       "9     0.019912\n",
       "15    0.011062\n",
       "14    0.008850\n",
       "7     0.006637\n",
       "8     0.004425\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data = pd.read_csv('data/arrhythmia.data',  header=None)\n",
    "\n",
    "df_data.groupby(df_data[279]).count().sort_values(0, ascending=False)[0]/len(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bcd39-f203-43cb-adfe-4c0ac0a11d33",
   "metadata": {},
   "source": [
    "There are 16 diffrent calsses of this data set, however, as we can see that about 54% percent of the instances are normal, the rest of samples are in different levels of disorder. In this case, it would be a great challange to classify all 16 levels in one go because the data is extremely imbalanced. Therefore, I would like to initially treat this problem as a binary classification, to predict whether a pateint is normal or has arraythmia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3901c2-eccb-4d93-86f2-ad5a15b1061a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 376), (11, 22), (10, 8), (12, 1), (14, 1), (0, 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_nan = {}\n",
    "for i in range(0, 279):\n",
    "    no_of_nan[i] = len(df_data[df_data[i]== '?'])\n",
    "sorted(no_of_nan.items(), key=lambda item: item[1], reverse=True)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99be0736-07cb-45df-b25d-ec930eeb8051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.831858407079646"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "376/len(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5bb7f-516f-4e5b-bf1d-0099acb8348b",
   "metadata": {},
   "source": [
    "Missing values are in column 11, 10, 12, 13, 14. \n",
    "we can see column 13 has over 83% missing data. \n",
    "These columns are Vector angles in degrees on front plane of:\n",
    "10 QRS\n",
    "11 T\n",
    "12 P\n",
    "13 QRST\n",
    "14 JAs \n",
    "Seems like they are related, therefore, I decide to drop all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26332afe-a512-451f-b44f-5d046302bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.drop(columns = [10, 11, 12, 13, 14], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12e2f5-0f0a-410e-b924-5596c5d4939c",
   "metadata": {},
   "source": [
    "Now, Lets build the new labels for the binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23aba244-042f-42f5-899f-606bd3718982",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = []\n",
    "#the last column records labels\n",
    "for label in df_data[279].values:\n",
    "    if label != 1:\n",
    "        new_labels.append(0)\n",
    "    else:\n",
    "        new_labels.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eaca61f-790f-4b32-87a2-dec02946065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I want to drop the label column to build the training data.\n",
    "df_data.drop(columns = 279, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aabf77f8-4f01-482a-875b-2728d3a93b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  269  270   271  272  \\\n",
       "0   75    0  190   80   91  193  371  174  121  -16  ... -0.3  0.0   9.0 -0.9   \n",
       "1   56    1  165   64   81  174  401  149   39   25  ... -0.5  0.0   8.5  0.0   \n",
       "2   54    0  172   95  138  163  386  185  102   96  ...  0.9  0.0   9.5 -2.4   \n",
       "3   55    0  175   94  100  202  380  179  143   28  ...  0.1  0.0  12.2 -2.2   \n",
       "4   75    0  190   80   88  181  360  177  103  -16  ... -0.4  0.0  13.1 -3.6   \n",
       "\n",
       "   273  274  275  276   277   278  \n",
       "0  0.0  0.0  0.9  2.9  23.3  49.4  \n",
       "1  0.0  0.0  0.2  2.1  20.4  38.8  \n",
       "2  0.0  0.0  0.3  3.4  12.3  49.0  \n",
       "3  0.0  0.0  0.4  2.6  34.6  61.6  \n",
       "4  0.0  0.0 -0.1  3.9  25.4  62.8  \n",
       "\n",
       "[5 rows x 274 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e14b922-de56-4ac0-9b98-27f905a15fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[75.0,0.0,190.0,8...|\n",
      "|    0|[56.0,1.0,165.0,6...|\n",
      "|    0|[54.0,0.0,172.0,9...|\n",
      "|    1|[55.0,0.0,175.0,9...|\n",
      "|    0|[75.0,0.0,190.0,8...|\n",
      "|    0|[13.0,0.0,169.0,5...|\n",
      "|    1|[40.0,1.0,160.0,5...|\n",
      "|    1|[49.0,1.0,162.0,5...|\n",
      "|    1|[44.0,0.0,168.0,5...|\n",
      "|    0|[50.0,1.0,167.0,6...|\n",
      "|    0|[62.0,0.0,170.0,7...|\n",
      "|    1|[45.0,1.0,165.0,8...|\n",
      "|    0|[54.0,1.0,172.0,5...|\n",
      "|    0|[30.0,0.0,170.0,7...|\n",
      "|    1|[44.0,1.0,160.0,8...|\n",
      "|    1|[47.0,1.0,150.0,4...|\n",
      "|    0|[47.0,0.0,171.0,5...|\n",
      "|    1|[46.0,1.0,158.0,5...|\n",
      "|    1|[73.0,0.0,165.0,6...|\n",
      "|    1|[57.0,1.0,166.0,7...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from pyspark.ml.linalg import Vectors\n",
    "def prepare_data(rawData, normalizer=None):\n",
    "    if normalizer:\n",
    "        return normalize(rawData, norm=normalizer)\n",
    "    else:\n",
    "        return rawData\n",
    "    \n",
    "array = prepare_data(df_data.to_numpy(), normalizer = None)\n",
    "list_tuples = []\n",
    "for i in range(0, len(array)):\n",
    "    list_tuples.append((new_labels[i],Vectors.dense(array[i])))\n",
    "data = spark.createDataFrame(list_tuples, [\"label\", \"features\"])\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c779fe7-70ee-4fe7-b678-835b1ef1a5f1",
   "metadata": {},
   "source": [
    "I also want to do feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6569ff56-fc23-4ce3-8774-4c89249443a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import UnivariateFeatureSelector\n",
    "selector = UnivariateFeatureSelector(outputCol=\"selectedFeatures\")\n",
    "selector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(100)\n",
    "model = selector.fit(data)\n",
    "model.getFeaturesCol()\n",
    "df_selected = model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5168c2c4-24fd-4b65-a918-0adaeb11a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|    selectedFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[75.0,0.0,190.0,8...|[0.0,91.0,174.0,4...|\n",
      "|[56.0,1.0,165.0,6...|[1.0,81.0,149.0,0...|\n",
      "|[54.0,0.0,172.0,9...|[0.0,138.0,185.0,...|\n",
      "|[55.0,0.0,175.0,9...|[0.0,100.0,179.0,...|\n",
      "|[75.0,0.0,190.0,8...|[0.0,88.0,177.0,4...|\n",
      "|[13.0,0.0,169.0,5...|[0.0,100.0,174.0,...|\n",
      "|[40.0,1.0,160.0,5...|[1.0,77.0,133.0,0...|\n",
      "|[49.0,1.0,162.0,5...|[1.0,78.0,157.0,3...|\n",
      "|[44.0,0.0,168.0,5...|[0.0,84.0,160.0,0...|\n",
      "|[50.0,1.0,167.0,6...|[1.0,89.0,156.0,4...|\n",
      "|[62.0,0.0,170.0,7...|[0.0,102.0,156.0,...|\n",
      "|[45.0,1.0,165.0,8...|[1.0,77.0,150.0,2...|\n",
      "|[54.0,1.0,172.0,5...|[1.0,78.0,163.0,0...|\n",
      "|[30.0,0.0,170.0,7...|[0.0,91.0,157.0,0...|\n",
      "|[44.0,1.0,160.0,8...|[1.0,77.0,163.0,0...|\n",
      "|[47.0,1.0,150.0,4...|[1.0,75.0,169.0,0...|\n",
      "|[47.0,0.0,171.0,5...|[0.0,82.0,169.0,0...|\n",
      "|[46.0,1.0,158.0,5...|[1.0,70.0,122.0,0...|\n",
      "|[73.0,0.0,165.0,6...|[0.0,91.0,175.0,5...|\n",
      "|[57.0,1.0,166.0,7...|[1.0,82.0,158.0,1...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected.select('features', 'selectedFeatures').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33411fc-af72-4459-bf5b-2f0c70d927d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'selectedFeatures', labelCol = 'label', maxDepth = 3)\n",
    "pipeline = Pipeline(stages=[dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bcc5de-849e-469c-bcd4-eb00bcf2a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependencies.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c56517-6704-48ed-b4c3-8fa2b3d050b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #5 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLogisticRegression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/Arrhythmia/app/dependencies/model.py:42\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     40\u001b[0m             arg_str_list\u001b[38;5;241m.\u001b[39mappend(params_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(value))\n\u001b[1;32m     41\u001b[0m     arg_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_str_list)\n\u001b[0;32m---> 42\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_model \u001b[38;5;241m=\u001b[39m dynamic_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #5 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "Model('LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43d435-f82a-4779-8670-b26a264c0e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71ece4cc9c4aede854b5983a138a0d4c9cc415fe9dd0477bef89310f7b90319c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
